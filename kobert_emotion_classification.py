# -*- coding: utf-8 -*-
"""KoBERT_Emotion_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fUXWFcrbwCrc-PwlMqxmfr6jBr9FTRtL

# KoBERT 감정 분류 모델

##1. 환경 세팅
"""

# !pip install torch torchvision torchaudio
# !pip install transformers==4.38.2 datasets tqdm
# !pip install kobert-transformers

import torch
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time

# GPU 셋팅
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("사용 디바이스:", device)

import os
base_path = "/content/drive/MyDrive/Colab Notebooks/LG_DX"
os.chdir(base_path)
os.getcwd()

"""##2. 데이터 로드 및 전처리"""

# 데이터 로드
df_train = pd.read_csv("train.csv")
df_val = pd.read_csv("val.csv")

# 라벨 매핑
label2id = {'기쁨': 0, '슬픔': 1, '분노': 2, '불안': 3, '평온': 4}
id2label = {v: k for k, v in label2id.items()}
df_train["label"] = df_train["emotion"].map(label2id)
df_val["label"] = df_val["emotion"].map(label2id)

print("train.csv shape:", df_train.shape)
print("val.csv shape:", df_val.shape)

"""##3. 토크나이저 및 데이터셋 클래스 정의"""

from transformers import AutoTokenizer, BertForSequenceClassification, BertConfig

model_name = "monologg/kobert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
config = BertConfig.from_pretrained(model_name, num_labels=5, hidden_dropout_prob=0.3)
model = BertForSequenceClassification.from_pretrained(model_name, config=config)
model.to(device)

def tokenize_fn(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")

class KoDataset(torch.utils.data.Dataset):
    def __init__(self, df):
        self.texts = df["text"].tolist()
        self.labels = df["label"].tolist()
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        item = tokenize_fn({"text": [self.texts[idx]]})
        return {
            "input_ids": item["input_ids"].squeeze(),
            "attention_mask": item["attention_mask"].squeeze(),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

# 부분 데이터 사용 시 각주 해제
# df_train = df_train.sample(n=35000, random_state=42)
# df_val = df_val.sample(n=15000, random_state=42)

train_dataset = KoDataset(df_train)
val_dataset = KoDataset(df_val)

"""##4. DataLoader, Optimizer"""

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True) # 32, 64, 128
val_loader = DataLoader(val_dataset, batch_size=128)
optimizer = AdamW(model.parameters(), lr=2e-5) # 1e-5, 2e-5, 3e-5, 5e-5

"""##5. 학습 & 검증 루프"""

patience = 5
counter = 0
best_f1 = 0.0
best_state = None

for epoch in range(1, 30):
    start_time = time.time()
    print(f"\n===== [Epoch {epoch}] =====")

    # ----- 1. Train -----
    model.train()
    total_loss = 0
    train_bar = tqdm(train_loader, desc="Train", leave=False)
    train_labels, train_preds = [], []
    for batch in train_bar:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"],
            labels=batch["labels"]
        )
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        total_loss += loss.item()
        train_bar.set_postfix(loss=f"{loss.item():.4f}")

        # For train accuracy
        logits = outputs.logits.detach().cpu().numpy()
        preds = logits.argmax(axis=1)
        labels = batch["labels"].detach().cpu().numpy()
        train_labels.extend(labels)
        train_preds.extend(preds)

    avg_train_loss = total_loss / len(train_loader)
    train_acc = accuracy_score(train_labels, train_preds)
    train_f1 = f1_score(train_labels, train_preds, average="macro")

    # ----- 2. Validation -----
    model.eval()
    val_labels, val_preds = [], []
    val_bar = tqdm(val_loader, desc="Valid", leave=False)
    with torch.no_grad():
        for batch in val_bar:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(
                input_ids=batch["input_ids"],
                attention_mask=batch["attention_mask"]
            )
            logits = outputs.logits
            preds = logits.argmax(dim=1).cpu().numpy()
            labels = batch["labels"].cpu().numpy()
            val_labels.extend(labels)
            val_preds.extend(preds)
    acc = accuracy_score(val_labels, val_preds)
    f1 = f1_score(val_labels, val_preds, average="macro")

    epoch_time = time.time() - start_time
    print(f"Epoch {epoch} | time: {epoch_time:.1f}s | train_loss: {avg_train_loss:.4f} | train_acc: {train_acc:.4f} | val_acc: {acc:.4f} | val_f1: {f1:.4f}")

    # ----- 3. 얼리스타핑 -----
    if f1 > best_f1:
        print(f">>> Best model updated at epoch {epoch}")
        best_f1 = f1
        best_state = model.state_dict()
        torch.save(best_state, "./best_kobert_model.pt")
        counter = 0
    else:
        counter += 1
        print(f"(EarlyStopping Counter: {counter} / {patience})")
        if counter >= patience:
            print("Early stopping triggered.")
            break

    if counter > 0:
        print(f"  ⏳ {patience - counter} patience left until stop.")

"""##6. 모델 로드 & 최종 성능 평가
- Accuracy
- F1-score
- Classification Report
- Confusion Matrix
"""

# 저장된 best 모델 불러오기
model.load_state_dict(torch.load("./best_kobert_model.pt", map_location=device))
model.eval()

val_labels, val_preds = [], []
with torch.no_grad():
    for batch in val_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(
            input_ids=batch["input_ids"],
            attention_mask=batch["attention_mask"]
        )
        logits = outputs.logits
        preds = logits.argmax(dim=1).cpu().numpy()
        labels = batch["labels"].cpu().numpy()
        val_labels.extend(labels)
        val_preds.extend(preds)

print("Validation Accuracy :", accuracy_score(val_labels, val_preds))
print("Validation F1_macro:", f1_score(val_labels, val_preds, average="macro"))
print(classification_report(val_labels, val_preds, target_names=label2id.keys(), labels=list(label2id.values())))

# Confusion Matrix 시각화
cm = confusion_matrix(val_labels, val_preds)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=label2id.keys(), yticklabels=label2id.keys())
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()